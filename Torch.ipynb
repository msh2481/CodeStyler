{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Torch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOVBEJUX06peTuYv/NTK+s5",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/msh2481/CodeStyler/blob/main/Torch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1II5jRa_xi2",
        "outputId": "9e715e25-81ba-4ba3-e3c1-951435d47019"
      },
      "source": [
        "!rm -rf ./*\n",
        "!git clone https://github.com/msh2481/CodeStyler.git && mv CodeStyler/* . && rm -rf CodeStyler\n",
        "!ls"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CodeStyler'...\n",
            "remote: Enumerating objects: 7936, done.\u001b[K\n",
            "remote: Counting objects: 100% (7936/7936), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6536/6536), done.\u001b[K\n",
            "remote: Total 7936 (delta 1405), reused 7917 (delta 1399), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (7936/7936), 9.12 MiB | 10.14 MiB/s, done.\n",
            "Resolving deltas: 100% (1405/1405), done.\n",
            "Baseline.ipynb\t   filenames.txt  README.md\t   Recurrent.ipynb\n",
            "Feedforward.ipynb  files\t  Reccurent.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lp_rIGE9DvEE"
      },
      "source": [
        "from random import shuffle, choices, choice\n",
        "from collections import deque, defaultdict, Counter\n",
        "from itertools import islice\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWx9mjSMGS1w"
      },
      "source": [
        "CHUNK_SIZE = 6\n",
        "BATCH_SIZE = 256\n",
        "BATCHES_IN_TRAIN = 10\n",
        "BATCHES_IN_TEST = 2\n",
        "TRAIN_SIZE = BATCH_SIZE * BATCHES_IN_TRAIN\n",
        "TEST_SIZE = BATCH_SIZE * BATCHES_IN_TEST\n",
        "MIN_OCCURENCES = 10\n",
        "MEMORY = 100"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMtsU1nro22g"
      },
      "source": [
        "def fmt(number):\n",
        "    return '{:.5f}'.format(number)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xU9muuIWBcxJ",
        "outputId": "e7cb3468-f525-4561-c1d4-e3ba4023d6a2"
      },
      "source": [
        "rawTexts = []\n",
        "alphabet = Counter()\n",
        "import string\n",
        "printable = set(string.printable)\n",
        "\n",
        "for filename in open('filenames.txt'):\n",
        "    if len(rawTexts) > TRAIN_SIZE + TEST_SIZE:\n",
        "        break\n",
        "    text = open(filename.strip(), encoding='utf-8').read()\n",
        "    text = ''.join([x for x in text if  x in printable])\n",
        "    if 'debug' in text or 'DEBUG' in text or '000' in text:\n",
        "        continue\n",
        "    for c in text:\n",
        "        assert c in printable\n",
        "    print(text)\n",
        "    alphabet.update(text)\n",
        "    for pos in range(0, len(text) - CHUNK_SIZE + 1):\n",
        "        rawTexts.append(text[pos : pos + CHUNK_SIZE])\n",
        "alphabetCount = Counter()\n",
        "alphabetCount['%'] = 0\n",
        "for x, y in alphabet.items():\n",
        "    if y >= MIN_OCCURENCES:\n",
        "        alphabetCount[x] += y\n",
        "    else:\n",
        "        alphabetCount['%'] += y\n",
        "alphabet = [x for x, y in alphabetCount.items()]\n",
        "ALPHABET_SIZE = len(alphabet)\n",
        "print(f'alphabet of length {len(alphabet)}: {alphabetCount}')\n",
        "\n",
        "shuffle(rawTexts)\n",
        "print(f'{len(rawTexts)} texts in total')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "// !LANGUAGE: +NewInference +ProhibitInvisibleAbstractMethodsInSuperclasses\n",
            "// !DIAGNOSTICS: -UNUSED_VARIABLE -ASSIGNED_BUT_NEVER_ACCESSED_VARIABLE -UNUSED_VALUE -UNUSED_PARAMETER -UNUSED_EXPRESSION\n",
            "// SKIP_TXT\n",
            "// FULL_JDK\n",
            "\n",
            "// MODULE: base\n",
            "// FILE: AbstractClassCase1.kt\n",
            "package base\n",
            "\n",
            "// TESTCASE NUMBER: 1\n",
            "abstract class AbstractClassCase1() {\n",
            "    <!INCOMPATIBLE_MODIFIERS!>private<!> <!INCOMPATIBLE_MODIFIERS!>abstract<!> fun priv()\n",
            "    protected abstract fun prot()\n",
            "    internal abstract fun int()\n",
            "    public abstract fun pub()\n",
            "\n",
            "    <!INCOMPATIBLE_MODIFIERS!>private<!> <!INCOMPATIBLE_MODIFIERS!>abstract<!> val priv1: String\n",
            "    protected abstract val prot1: String\n",
            "    internal abstract val int1: String\n",
            "    public abstract val pub1: String\n",
            "}\n",
            "\n",
            "<!INVISIBLE_ABSTRACT_MEMBER_FROM_SUPER_ERROR!>class Case1<!> : AbstractClassCase1(){\n",
            "    override fun prot() {}\n",
            "\n",
            "    override fun int() {\n",
            "        prot()\n",
            "    }\n",
            "\n",
            "    override fun pub() {}\n",
            "\n",
            "    override val prot1: String\n",
            "        get() = \"\"\n",
            "    override val int1: String\n",
            "        get() = \"\"\n",
            "    override val pub1: String\n",
            "        get() = \"\"\n",
            "\n",
            "}\n",
            "\n",
            "fun case1(){\n",
            "    val a = Case1()\n",
            "    a.<!INVISIBLE_REFERENCE!>priv<!>()\n",
            "    a.<!INVISIBLE_REFERENCE!>prot<!>()\n",
            "    a.int()\n",
            "    a.pub()\n",
            "\n",
            "    a.<!INVISIBLE_REFERENCE!>priv1<!>\n",
            "    a.<!INVISIBLE_REFERENCE!>prot1<!>\n",
            "    a.int1\n",
            "    a.pub1\n",
            "}\n",
            "\n",
            "//MODULE: implBase(base)\n",
            "//FILE: Impl.kt\n",
            "package implBase\n",
            "import base.*\n",
            "\n",
            "// TESTCASE NUMBER: 2\n",
            "fun case2() {\n",
            "    val a = Case1()\n",
            "    a.<!INVISIBLE_REFERENCE!>priv<!>()\n",
            "    a.<!INVISIBLE_REFERENCE!>prot<!>()\n",
            "    a.<!INVISIBLE_REFERENCE!>int<!>()\n",
            "    a.pub()\n",
            "\n",
            "    a.<!INVISIBLE_REFERENCE!>priv1<!>\n",
            "    a.<!INVISIBLE_REFERENCE!>prot1<!>\n",
            "    a.<!INVISIBLE_REFERENCE!>int1<!>\n",
            "    a.pub1\n",
            "}\n",
            "\n",
            "// !LANGUAGE: +NewInference +ProhibitInvisibleAbstractMethodsInSuperclasses\n",
            "// !DIAGNOSTICS: -UNUSED_VARIABLE -ASSIGNED_BUT_NEVER_ACCESSED_VARIABLE -UNUSED_VALUE -UNUSED_PARAMETER -UNUSED_EXPRESSION\n",
            "// SKIP_TXT\n",
            "// FULL_JDK\n",
            "\n",
            "// MODULE: libModule\n",
            "// FILE: libModule/BaseJava1.java\n",
            "package libModule;\n",
            "\n",
            "public abstract class BaseJava1 {\n",
            "    /*package-private*/ abstract void boojava();\n",
            "}\n",
            "\n",
            "\n",
            "// FILE: BaseKotlin1.kt\n",
            "package libModule\n",
            "\n",
            "public abstract class BaseKotlin1 {\n",
            "    internal abstract fun bookotlin()\n",
            "}\n",
            "\n",
            "// MODULE: mainModule(libModule)\n",
            "// FILE: JavaClassWithAbstractKotlinClass.java\n",
            "package mainModule\n",
            "import libModule.*\n",
            "\n",
            "/*\n",
            " * TESTCASE NUMBER: 1\n",
            " * UNEXPECTED BEHAVIOUR\n",
            " * ISSUES: KT-35325\n",
            " */\n",
            "public class JavaClassWithAbstractKotlinClass {\n",
            "\n",
            "    public void zoo()\n",
            "    {\n",
            "        //todo: INVISIBLE_ABSTRACT_MEMBER_FROM_SUPER should be expected\n",
            "        BaseKotlin1 baseKotlin = new BaseKotlin1() {};\n",
            "\n",
            "        BaseKotlin1 baseKotlin = new BaseKotlinImpl();\n",
            "        baseKotlin.bookotlin$libModule(); //highlight Error: usage of Kotlin internal declaration from different module\n",
            "\n",
            "        BaseKotlinImpl baseKotlinImpl = new BaseKotlinImpl();\n",
            "        baseKotlinImpl.bookotlin$libModule();  //there is no any Error, but should be\n",
            "    }\n",
            "\n",
            "    class BaseKotlinImpl extends BaseKotlin1 {\n",
            "        @Override\n",
            "        public void bookotlin$libModule() {\n",
            "            System.out.println(\"wefrgth\");\n",
            "        }\n",
            "    }\n",
            "\n",
            "}\n",
            "\n",
            "// FILE: KotlinClassWithAbstractJavaClass.kt\n",
            "package mainModule\n",
            "import libModule.*\n",
            "\n",
            "// TESTCASE NUMBER: 2\n",
            "class KotlinClassWithAbstractJavaClass() {\n",
            "    fun foo() {\n",
            "        val baseJava1 = <!INVISIBLE_ABSTRACT_MEMBER_FROM_SUPER_ERROR!>object<!> : BaseJava1() {}\n",
            "        val baseKotlin = <!INVISIBLE_ABSTRACT_MEMBER_FROM_SUPER_ERROR!>object<!> : BaseKotlin1() {}\n",
            "    }\n",
            "}\n",
            "\n",
            "alphabet of length 57: Counter({' ': 514, 'a': 169, 'E': 140, 't': 137, '\\n': 136, 'e': 120, 's': 109, 'l': 108, 'i': 107, 'o': 94, 'I': 93, 'r': 88, 'n': 81, 'S': 73, 'b': 72, '%': 67, 'R': 66, 'p': 58, '!': 55, 'c': 55, '/': 53, 'N': 53, '_': 53, 'B': 53, 'A': 50, 'u': 47, 'C': 46, 'M': 45, 'v': 43, '(': 41, ')': 41, 'L': 40, 'U': 39, 'd': 35, '1': 35, '<': 34, '>': 34, 'T': 32, ':': 31, '.': 29, 'K': 27, 'F': 26, 'D': 25, 'O': 25, 'V': 23, 'g': 21, 'f': 19, '{': 18, '}': 18, 'm': 18, 'P': 17, 'h': 15, 'k': 15, '-': 12, 'J': 10, '=': 10, '*': 10})\n",
            "3475 texts in total\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4y1ZhRmUF3A"
      },
      "source": [
        "charToIndexMap = { c : i for i, c in enumerate(alphabet) }\n",
        "def charToIndex(c):\n",
        "    return torch.as_tensor(charToIndexMap.get(c, ALPHABET_SIZE - 1), dtype=torch.long)\n",
        "\n",
        "def stringToTensor(cur):\n",
        "    x = torch.zeros(size=(len(cur), ALPHABET_SIZE))\n",
        "    for j in range(len(cur)):\n",
        "        x[j][charToIndex(cur[j])] = 1\n",
        "    return x\n",
        "\n",
        "class StringDataset(Dataset):\n",
        "    def __init__(self, strings):\n",
        "        super(StringDataset, self).__init__()\n",
        "        self.strings = strings\n",
        "    def __len__(self):\n",
        "        return len(self.strings)\n",
        "    def __getitem__(self, i):\n",
        "        return stringToTensor(self.strings[i])\n",
        "\n",
        "trainSet = DataLoader(StringDataset(rawTexts[: TRAIN_SIZE]), batch_size=BATCH_SIZE, shuffle=True)\n",
        "testSet = DataLoader(StringDataset(rawTexts[TRAIN_SIZE : TRAIN_SIZE + TEST_SIZE]), batch_size=BATCH_SIZE, shuffle=False)\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oniJOQwuVuWw",
        "outputId": "36ba90d5-3894-4758-a679-70aa8af2e2c5"
      },
      "source": [
        "print(len(trainSet), len(testSet))\n",
        "print('---')\n",
        "# print(next(iter(trainSet)))\n",
        "print('---')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10 2\n",
            "---\n",
            "---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l52z5m8oISM4"
      },
      "source": [
        ""
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHDr-qL-a8L5"
      },
      "source": [
        "lossFunction = nn.CrossEntropyLoss()\n",
        "\n",
        "def parametersTensor(predictor):\n",
        "    return torch.cat(tuple(elem.view(-1) for elem in predictor.parameters()))\n",
        "\n",
        "def gradientsTensor(predictor):\n",
        "    return torch.cat(tuple(elem.grad.view(-1) for elem in predictor.parameters()))\n",
        "\n",
        "X_ORT = None\n",
        "Y_ORT = None\n",
        "\n",
        "def tensorTo2D(v):\n",
        "    global X_ORT, Y_ORT\n",
        "    if X_ORT is None:\n",
        "        assert Y_ORT is None\n",
        "        X_ORT = torch.rand(v.shape, dtype=torch.double)\n",
        "        Y_ORT = torch.rand(v.shape, dtype=torch.double)\n",
        "        X_ORT = F.normalize(X_ORT, dim=0)\n",
        "        Y_ORT = F.normalize(Y_ORT, dim=0)\n",
        "    print(X_ORT.sum(), X_ORT.mean(), X_ORT.std())\n",
        "    vx = torch.mul(v, X_ORT)\n",
        "    print(vx.mean(), vx.std())\n",
        "    vy = torch.mul(v, Y_ORT)\n",
        "    return vx.sum(), vy.sum()\n",
        "\n",
        "def evaluateOnBatch(predictor, batch):\n",
        "    N = batch.shape[0]\n",
        "    assert batch.shape == (N, CHUNK_SIZE, ALPHABET_SIZE)\n",
        "    h0 = torch.randn((1, N, ALPHABET_SIZE + MEMORY))\n",
        "    data = batch[:, :-1, :]\n",
        "    answer = batch[:, -1, :].argmax(dim=-1)\n",
        "    assert answer.shape == (N, )\n",
        "    output = predictor(data, h0)[0][:, -1, :]\n",
        "    output = output[:, :ALPHABET_SIZE]\n",
        "    assert output.shape == (N, ALPHABET_SIZE)\n",
        "    loss = lossFunction(output, answer)\n",
        "    accuracy = (output.argmax(dim=-1) == answer).float().mean()\n",
        "    return accuracy, loss\n",
        "        \n",
        "def train(predictor, optimizer, startEpoch):\n",
        "    predictor.train()\n",
        "    trainAccuracy = 0\n",
        "    trainLogLoss = 0\n",
        "    trainSize = 0\n",
        "    for batch in islice(trainSet, BATCHES_IN_TRAIN):\n",
        "        optimizer.zero_grad()\n",
        "        accuracy, loss = evaluateOnBatch(predictor, batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        trainAccuracy += accuracy\n",
        "        trainLogLoss += loss.item()\n",
        "    trainAccuracy /= BATCHES_IN_TRAIN\n",
        "    trainLogLoss /= BATCHES_IN_TRAIN\n",
        "\n",
        "    with torch.no_grad():\n",
        "        predictor.eval()\n",
        "        testAccuracy = 0\n",
        "        testLogLoss = 0\n",
        "        testSize = 0\n",
        "        for batch in islice(testSet, BATCHES_IN_TEST):\n",
        "            accuracy, logLoss = evaluateOnBatch(predictor, batch)\n",
        "            testAccuracy += accuracy\n",
        "            testLogLoss += loss.item()\n",
        "        testAccuracy /= BATCHES_IN_TEST\n",
        "        testLogLoss /= BATCHES_IN_TEST\n",
        "\n",
        "        p = parametersTensor(predictor)\n",
        "        g = gradientsTensor(predictor)\n",
        "        print(f'State: parameters = ({fmt(p.mean())}, {fmt(p.std())}) gradients = ({fmt(g.mean())},  {fmt(g.std())})')\n",
        "        print(f'#{startEpoch}: {fmt(trainAccuracy)} {fmt(trainLogLoss)} {fmt(testAccuracy)} {fmt(testLogLoss)}')\n",
        "        print(flush=True)"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXs0InGgwC37"
      },
      "source": [
        "def guessNext(predictor, text):\n",
        "    data = stringToTensor(text).view(1, -1, ALPHABET_SIZE)\n",
        "    h0 = torch.randn((1, 1, ALPHABET_SIZE + MEMORY))\n",
        "    output = predictor(data, h0)[0][:, -1, :]\n",
        "    output = output[0, :ALPHABET_SIZE]\n",
        "    return output\n",
        "\n",
        "def guessNextK(predictor, prefix, k):\n",
        "    for it in range(k):\n",
        "        p = guessNext(predictor, prefix)\n",
        "        i = p.argmax(dim=0).item()\n",
        "        c = alphabet[i]\n",
        "        prefix += c\n",
        "    return prefix"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxpBDZhhMwSx",
        "outputId": "ec3004fe-e7d8-45b1-e34b-d7be38ed6f73"
      },
      "source": [
        "predictor = nn.GRU(input_size=ALPHABET_SIZE,\n",
        "                   hidden_size=ALPHABET_SIZE+MEMORY,\n",
        "                   num_layers=1,\n",
        "                   bias=True,\n",
        "                   batch_first=True,\n",
        "                   dropout=0,\n",
        "                   bidirectional=False)\n",
        "optimizer = torch.optim.Adam(predictor.parameters())\n",
        "\n",
        "for i in range(10 ** 9):\n",
        "    train(predictor, optimizer, i)\n",
        "    print(guessNextK(predictor, choice(rawTexts), 100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State: parameters = (-0.00039, 0.04620) gradients = (0.00000,  0.00080)\n",
            "#0: 0.02617 4.03544 0.04297 4.01374\n",
            "\n",
            "// !DI >  r                                                                                               \n",
            "State: parameters = (-0.00055, 0.04655) gradients = (0.00000,  0.00074)\n",
            "#1: 0.08711 3.99345 0.11523 3.96973\n",
            "\n",
            "NUSED_                                                                                                    \n",
            "State: parameters = (-0.00068, 0.04712) gradients = (0.00000,  0.00072)\n",
            "#2: 0.13438 3.94480 0.14648 3.92562\n",
            "\n",
            "()\n",
            "                                                                                                       \n",
            "State: parameters = (-0.00082, 0.04804) gradients = (0.00000,  0.00067)\n",
            "#3: 0.14609 3.86899 0.14648 3.83229\n",
            "\n",
            "FROM_S                                                                                                    \n",
            "State: parameters = (-0.00101, 0.04942) gradients = (0.00000,  0.00053)\n",
            "#4: 0.14766 3.76935 0.14453 3.75009\n",
            "\n",
            ": 2\n",
            "cl                                                                                                    \n",
            "State: parameters = (-0.00120, 0.05084) gradients = (-0.00000,  0.00049)\n",
            "#5: 0.14687 3.70351 0.14258 3.66960\n",
            "\n",
            "e\n",
            "\n",
            "                                                                                                       \n",
            "State: parameters = (-0.00133, 0.05194) gradients = (-0.00000,  0.00057)\n",
            "#6: 0.14727 3.67944 0.14453 3.70188\n",
            "\n",
            "NVISIB                                                                                                    \n",
            "State: parameters = (-0.00142, 0.05293) gradients = (-0.00000,  0.00048)\n",
            "#7: 0.14727 3.65633 0.14453 3.64352\n",
            "\n",
            "aClass                                                                                                    \n",
            "State: parameters = (-0.00148, 0.05404) gradients = (0.00000,  0.00056)\n",
            "#8: 0.14727 3.63950 0.14453 3.63501\n",
            "\n",
            "MEMBER                                                                                                    \n",
            "State: parameters = (-0.00155, 0.05532) gradients = (-0.00001,  0.00060)\n",
            "#9: 0.14805 3.61386 0.14648 3.55926\n",
            "\n",
            "aseKot                                                                                                    \n",
            "State: parameters = (-0.00161, 0.05684) gradients = (0.00000,  0.00064)\n",
            "#10: 0.14805 3.58477 0.14453 3.56494\n",
            "\n",
            " {}\n",
            "\n",
            "                                                                                                     \n",
            "State: parameters = (-0.00166, 0.05853) gradients = (0.00000,  0.00061)\n",
            "#11: 0.14844 3.55028 0.14453 3.57935\n",
            "\n",
            "abstra                                                                                                    \n",
            "State: parameters = (-0.00172, 0.06041) gradients = (-0.00001,  0.00060)\n",
            "#12: 0.15000 3.51559 0.14844 3.45080\n",
            "\n",
            "\n",
            "// !D                                                                                                    \n",
            "State: parameters = (-0.00177, 0.06239) gradients = (-0.00001,  0.00059)\n",
            "#13: 0.15156 3.48400 0.14648 3.48899\n",
            "\n",
            "CE!>in                                                                                                    \n",
            "State: parameters = (-0.00182, 0.06441) gradients = (-0.00000,  0.00073)\n",
            "#14: 0.15586 3.44512 0.14844 3.43032\n",
            "\n",
            "\n",
            "\n",
            "// M                                                                                                    \n",
            "State: parameters = (-0.00186, 0.06641) gradients = (0.00000,  0.00075)\n",
            "#15: 0.15469 3.41641 0.15430 3.43357\n",
            "\n",
            "t libM                                                                                                    \n",
            "State: parameters = (-0.00187, 0.06837) gradients = (-0.00000,  0.00070)\n",
            "#16: 0.15352 3.38524 0.14844 3.40146\n",
            "\n",
            "Superc                                                                                                    \n",
            "State: parameters = (-0.00193, 0.07026) gradients = (-0.00001,  0.00076)\n",
            "#17: 0.15508 3.36064 0.14844 3.31704\n",
            "\n",
            "rride                                                                                                     \n",
            "State: parameters = (-0.00195, 0.07210) gradients = (-0.00001,  0.00071)\n",
            "#18: 0.15937 3.33177 0.15820 3.33565\n",
            "\n",
            "oo()\n",
            "                                                                                                     \n",
            "State: parameters = (-0.00194, 0.07385) gradients = (-0.00000,  0.00054)\n",
            "#19: 0.16055 3.30955 0.14453 3.25963\n",
            "\n",
            "e1(){\n",
            "                                                                                                    \n",
            "State: parameters = (-0.00193, 0.07561) gradients = (0.00000,  0.00085)\n",
            "#20: 0.15742 3.29167 0.15039 3.26609\n",
            "\n",
            "   a.<                                                                                                    \n",
            "State: parameters = (-0.00190, 0.07731) gradients = (-0.00000,  0.00083)\n",
            "#21: 0.15547 3.26853 0.14844 3.25465\n",
            "\n",
            "thodsI                                                                                                    \n",
            "State: parameters = (-0.00190, 0.07902) gradients = (0.00001,  0.00090)\n",
            "#22: 0.15508 3.24044 0.14844 3.24802\n",
            "\n",
            "  inte                                                                                                    \n",
            "State: parameters = (-0.00190, 0.08072) gradients = (-0.00001,  0.00069)\n",
            "#23: 0.16133 3.22129 0.15430 3.21984\n",
            "\n",
            "\n",
            "\n",
            "// M                                                                                                    \n",
            "State: parameters = (-0.00189, 0.08243) gradients = (-0.00001,  0.00105)\n",
            "#24: 0.15664 3.20369 0.14648 3.17626\n",
            "\n",
            "   pub                                                                                                    \n",
            "State: parameters = (-0.00191, 0.08414) gradients = (-0.00001,  0.00094)\n",
            "#25: 0.15664 3.17839 0.14844 3.18462\n",
            "\n",
            "thAbst                                                                                                    \n",
            "State: parameters = (-0.00196, 0.08586) gradients = (-0.00002,  0.00095)\n",
            "#26: 0.15469 3.15897 0.15234 3.19575\n",
            "\n",
            " base.                                                                                                    \n",
            "State: parameters = (-0.00201, 0.08756) gradients = (0.00001,  0.00082)\n",
            "#27: 0.15508 3.13490 0.14648 3.11806\n",
            "\n",
            "IBLE_M                                                                                                    \n",
            "State: parameters = (-0.00207, 0.08924) gradients = (0.00000,  0.00081)\n",
            "#28: 0.15547 3.11558 0.14844 3.03465\n",
            "\n",
            "TCASE                                                                                                     \n",
            "State: parameters = (-0.00213, 0.09092) gradients = (0.00000,  0.00096)\n",
            "#29: 0.15625 3.09617 0.15039 3.11977\n",
            "\n",
            "val ba                                                                                                    \n",
            "State: parameters = (-0.00219, 0.09256) gradients = (0.00002,  0.00105)\n",
            "#30: 0.15547 3.07921 0.14648 3.15607\n",
            "\n",
            " Abstr                                                                                                    \n",
            "State: parameters = (-0.00225, 0.09416) gradients = (-0.00002,  0.00100)\n",
            "#31: 0.15547 3.05620 0.14844 3.02425\n",
            "\n",
            "NVISIB                                                                                                    \n",
            "State: parameters = (-0.00230, 0.09570) gradients = (0.00000,  0.00114)\n",
            "#32: 0.15391 3.04289 0.14648 3.03570\n",
            "\n",
            "seKotl                                                                                                    \n",
            "State: parameters = (-0.00236, 0.09721) gradients = (0.00001,  0.00078)\n",
            "#33: 0.15352 3.02346 0.14648 2.96982\n",
            "\n",
            "K\n",
            "\n",
            "//                                                                                                     \n",
            "State: parameters = (-0.00244, 0.09870) gradients = (-0.00001,  0.00070)\n",
            "#34: 0.15508 3.00625 0.14844 2.93297\n",
            "\n",
            "\"\"\n",
            "\n",
            "}\n",
            "                                                                                                    \n",
            "State: parameters = (-0.00249, 0.10014) gradients = (-0.00001,  0.00070)\n",
            "#35: 0.15703 2.99454 0.14453 2.98707\n",
            "\n",
            " MODUL                                                                                                    \n",
            "State: parameters = (-0.00256, 0.10151) gradients = (-0.00001,  0.00101)\n",
            "#36: 0.15352 2.97603 0.14453 2.94670\n",
            "\n",
            "actCla                                                                                                    \n",
            "State: parameters = (-0.00265, 0.10288) gradients = (-0.00001,  0.00086)\n",
            "#37: 0.15508 2.96310 0.14648 3.01237\n",
            "\n",
            "rt lib                                                                                                    \n",
            "State: parameters = (-0.00271, 0.10420) gradients = (0.00001,  0.00087)\n",
            "#38: 0.15625 2.95425 0.14844 2.92037\n",
            "\n",
            "ED BEH                                                                                                    \n",
            "State: parameters = (-0.00281, 0.10547) gradients = (0.00001,  0.00113)\n",
            "#39: 0.15859 2.93880 0.14453 2.89210\n",
            "\n",
            "JavaCl                                                                                                    \n",
            "State: parameters = (-0.00288, 0.10674) gradients = (0.00000,  0.00104)\n",
            "#40: 0.15391 2.92555 0.14648 2.88853\n",
            "\n",
            "FERENC                                                                                                    \n",
            "State: parameters = (-0.00299, 0.10799) gradients = (-0.00001,  0.00072)\n",
            "#41: 0.15508 2.90945 0.14648 2.86288\n",
            "\n",
            "new Ba                                                                                                    \n",
            "State: parameters = (-0.00308, 0.10922) gradients = (-0.00001,  0.00102)\n",
            "#42: 0.15273 2.90090 0.14844 2.88918\n",
            "\n",
            "ARIABL                                                                                                    \n",
            "State: parameters = (-0.00316, 0.11043) gradients = (0.00001,  0.00102)\n",
            "#43: 0.15820 2.88891 0.14648 2.97470\n",
            "\n",
            "rror:                                                                                                     \n",
            "State: parameters = (-0.00326, 0.11159) gradients = (0.00000,  0.00090)\n",
            "#44: 0.15898 2.87682 0.15820 2.89663\n",
            "\n",
            "\n",
            "// FU                                                                                                    \n",
            "State: parameters = (-0.00335, 0.11274) gradients = (-0.00001,  0.00089)\n",
            "#45: 0.15469 2.86697 0.15234 2.91192\n",
            "\n",
            "avaCla                                                                                                    \n",
            "State: parameters = (-0.00346, 0.11387) gradients = (0.00000,  0.00099)\n",
            "#46: 0.15703 2.85770 0.14844 2.88022\n",
            "\n",
            "   ove                                                                                                    \n",
            "State: parameters = (-0.00356, 0.11498) gradients = (0.00002,  0.00107)\n",
            "#47: 0.15547 2.84702 0.15234 2.80203\n",
            "\n",
            "act fu                                                                                                    \n",
            "State: parameters = (-0.00367, 0.11606) gradients = (0.00001,  0.00073)\n",
            "#48: 0.15781 2.83685 0.15039 2.84965\n",
            "\n",
            "otlin$                                                                                                    \n",
            "State: parameters = (-0.00377, 0.11710) gradients = (-0.00000,  0.00083)\n",
            "#49: 0.16016 2.83135 0.14453 2.85417\n",
            "\n",
            "\n",
            "// FU                                                                                                    \n",
            "State: parameters = (-0.00387, 0.11815) gradients = (-0.00001,  0.00101)\n",
            "#50: 0.15820 2.82227 0.15039 2.85623\n",
            "\n",
            "ass Ko                                                                                                    \n",
            "State: parameters = (-0.00398, 0.11916) gradients = (0.00001,  0.00123)\n",
            "#51: 0.15859 2.81208 0.15234 2.88205\n",
            "\n",
            "  over                                                                                                    \n",
            "State: parameters = (-0.00409, 0.12014) gradients = (-0.00000,  0.00085)\n",
            "#52: 0.15898 2.80612 0.15039 2.80454\n",
            "\n",
            "FERENC                                                                                                    \n",
            "State: parameters = (-0.00422, 0.12114) gradients = (-0.00001,  0.00080)\n",
            "#53: 0.15625 2.79822 0.15234 2.82008\n",
            "\n",
            "t1<!>\n",
            "                                                                                                    \n",
            "State: parameters = (-0.00435, 0.12211) gradients = (0.00001,  0.00069)\n",
            "#54: 0.16250 2.78942 0.15430 2.69563\n",
            "\n",
            "ternal                                                                                                    \n",
            "State: parameters = (-0.00448, 0.12305) gradients = (-0.00001,  0.00078)\n",
            "#55: 0.16523 2.78023 0.15234 2.77202\n",
            "\n",
            "in1.kt                                                                                                    \n",
            "State: parameters = (-0.00458, 0.12400) gradients = (-0.00001,  0.00076)\n",
            "#56: 0.16523 2.77528 0.16016 2.72340\n",
            "\n",
            "le/Bas                                                                                                    \n",
            "State: parameters = (-0.00469, 0.12493) gradients = (-0.00001,  0.00084)\n",
            "#57: 0.16367 2.76701 0.15820 2.79345\n",
            "\n",
            "\n",
            "//MODILL                                                                                                 \n",
            "State: parameters = (-0.00481, 0.12582) gradients = (-0.00001,  0.00088)\n",
            "#58: 0.16875 2.76216 0.16406 2.74106\n",
            "\n",
            "ule;\n",
            "\n",
            "                                                                                                    \n",
            "State: parameters = (-0.00490, 0.12672) gradients = (-0.00001,  0.00085)\n",
            "#59: 0.17148 2.75445 0.16211 2.74784\n",
            "\n",
            " publi                                                                                                    \n",
            "State: parameters = (-0.00499, 0.12760) gradients = (-0.00002,  0.00078)\n",
            "#60: 0.17500 2.74863 0.16797 2.73239\n",
            "\n",
            "se1.kt                                                                                                    \n",
            "State: parameters = (-0.00509, 0.12846) gradients = (-0.00000,  0.00090)\n",
            "#61: 0.18984 2.74316 0.18945 2.69691\n",
            "\n",
            "t1: St                                                                                                    \n",
            "State: parameters = (-0.00519, 0.12932) gradients = (-0.00000,  0.00084)\n",
            "#62: 0.21250 2.73769 0.23438 2.76521\n",
            "\n",
            " case1                                                                                                    \n",
            "State: parameters = (-0.00529, 0.13015) gradients = (0.00001,  0.00078)\n",
            "#63: 0.26328 2.73002 0.26562 2.73766\n",
            "\n",
            "al abs                                                                                                    \n",
            "State: parameters = (-0.00537, 0.13101) gradients = (-0.00001,  0.00062)\n",
            "#64: 0.32500 2.72388 0.30859 2.72197\n",
            "\n",
            "ystem.                                                                                                    \n",
            "State: parameters = (-0.00551, 0.13186) gradients = (0.00000,  0.00076)\n",
            "#65: 0.34531 2.71720 0.28906 2.74174\n",
            "\n",
            "PER_EREEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE\n",
            "State: parameters = (-0.00564, 0.13270) gradients = (0.00001,  0.00070)\n",
            "#66: 0.34141 2.70796 0.29492 2.67341\n",
            "\n",
            "rt libMt                                                                                                  \n",
            "State: parameters = (-0.00575, 0.13354) gradients = (-0.00001,  0.00076)\n",
            "#67: 0.33750 2.70277 0.31250 2.76320\n",
            "\n",
            "al pro l                                                                                                  \n",
            "State: parameters = (-0.00586, 0.13438) gradients = (-0.00001,  0.00101)\n",
            "#68: 0.35586 2.69664 0.33008 2.67353\n",
            "\n",
            "INVISIBLE_RERERENCESSEENEESEENEESEENEESEENEESEENEESEENEESEENEESEENEESEENEESEENEESEENEESEENEESEENEESEENEESE\n",
            "State: parameters = (-0.00596, 0.13519) gradients = (-0.00002,  0.00082)\n",
            "#69: 0.38633 2.69181 0.33203 2.67995\n",
            "\n",
            "/ MODULE_ EE_EEEEEEEENEESEENEESEENEESEENEESEENEESEENEESEENEESEENEESEENEESEENEESEENEESEENEESEENEESEENEESEEN\n",
            "State: parameters = (-0.00604, 0.13602) gradients = (0.00001,  0.00078)\n",
            "#70: 0.38711 2.68523 0.36719 2.72024\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "975CmTDzyF_T"
      },
      "source": [
        "optimizer = torch.optim.SGD(predictor.parameters(), lr=0.1, momentum=0.9)\n",
        "\n",
        "for i in range(10 ** 9):\n",
        "    train(predictor, optimizer, i)\n",
        "    print(i)\n",
        "    samplePrediction(predictor, 64)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}